[
  {
    "timestamp": 1763310200.6776462,
    "tinystories_sequences": 16000,
    "custom_sequences": 0,
    "base_sequences": 0,
    "total_sequences": 0,
    "tinystories_weight": 0.3,
    "limit_custom_examples": null,
    "test_split_ratio": 0.4,
    "per_path_counts": {},
    "custom_eval_count": 0,
    "hyperparameters": {
      "embed_size": 1024,
      "block_size": 1024,
      "batch_size": 16,
      "num_epochs": 3,
      "learning_rate": 0.001,
      "kgram_k": 3,
      "kgram_chunk_size": 1
    },
    "model_metrics": {
      "lstm_seq": {
        "epoch_losses": [
          3.528465428352356,
          2.1820184636116027,
          1.880254989862442
        ],
        "final_loss": 1.880254989862442,
        "total_steps": 150,
        "elapsed": 18090.486329078674,
        "eval_losses": [
          3.262684808731079,
          2.796768044948578,
          2.5905557994842527
        ],
        "final_eval_loss": 2.5905557994842527
      },
      "transformer_gptoss": {
        "epoch_losses": [
          2.930899877548218,
          1.9670565390586854,
          1.8790086126327514
        ],
        "final_loss": 1.8790086126327514,
        "total_steps": 150,
        "elapsed": 38473.07559108734,
        "eval_losses": [
          2.9558183126449586,
          2.64672233915329,
          2.459734257698059
        ],
        "final_eval_loss": 2.459734257698059
      }
    },
    "prompt": "Once upon a"
  },
  {
    "timestamp": 1763310337.7928581,
    "tinystories_sequences": 16000,
    "custom_sequences": 0,
    "base_sequences": 0,
    "total_sequences": 0,
    "tinystories_weight": 1.0,
    "limit_custom_examples": null,
    "test_split_ratio": 0.4,
    "per_path_counts": {},
    "custom_eval_count": 0,
    "hyperparameters": {
      "embed_size": 1024,
      "block_size": 1024,
      "batch_size": 16,
      "num_epochs": 3,
      "learning_rate": 0.001,
      "kgram_k": 3,
      "kgram_chunk_size": 1
    },
    "model_metrics": {
      "lstm_seq": {
        "epoch_losses": [
          3.176455042362213,
          2.14539261341095,
          1.8949462699890136
        ],
        "final_loss": 1.8949462699890136,
        "total_steps": 150,
        "elapsed": 18377.693028211594,
        "eval_losses": [
          3.2527497539520263,
          2.7810194416046143,
          2.5780593585968017
        ],
        "final_eval_loss": 2.5780593585968017
      },
      "transformer_gptoss": {
        "epoch_losses": [
          2.9772066402435304,
          2.090820300579071,
          1.8229141998291016
        ],
        "final_loss": 1.8229141998291016,
        "total_steps": 150,
        "elapsed": 38323.63927984238,
        "eval_losses": [
          2.9435942134857176,
          2.6287428913116453,
          2.448556935310364
        ],
        "final_eval_loss": 2.448556935310364
      }
    },
    "prompt": "Once upon a"
  },
  {
    "timestamp": 1763310425.154762,
    "tinystories_sequences": 16000,
    "custom_sequences": 0,
    "base_sequences": 0,
    "total_sequences": 0,
    "tinystories_weight": 0.5,
    "limit_custom_examples": null,
    "test_split_ratio": 0.4,
    "per_path_counts": {},
    "custom_eval_count": 0,
    "hyperparameters": {
      "embed_size": 1024,
      "block_size": 1024,
      "batch_size": 16,
      "num_epochs": 3,
      "learning_rate": 0.001,
      "kgram_k": 3,
      "kgram_chunk_size": 1
    },
    "model_metrics": {
      "lstm_seq": {
        "epoch_losses": [
          3.2147477531433104,
          2.2739643979072572,
          1.904613004922867
        ],
        "final_loss": 1.904613004922867,
        "total_steps": 150,
        "elapsed": 18123.990045309067,
        "eval_losses": [
          3.228124338150024,
          2.7554773516654967,
          2.548117672920227
        ],
        "final_eval_loss": 2.548117672920227
      },
      "transformer_gptoss": {
        "epoch_losses": [
          2.886679210662842,
          1.9112037134170532,
          1.9151509022712707
        ],
        "final_loss": 1.9151509022712707,
        "total_steps": 150,
        "elapsed": 38666.73880124092,
        "eval_losses": [
          2.916342357635498,
          2.6007868213653564,
          2.443449124336243
        ],
        "final_eval_loss": 2.443449124336243
      }
    },
    "prompt": "Once upon a"
  },
  {
    "timestamp": 1763310774.164809,
    "tinystories_sequences": 16000,
    "custom_sequences": 0,
    "base_sequences": 0,
    "total_sequences": 0,
    "tinystories_weight": 0.9,
    "limit_custom_examples": null,
    "test_split_ratio": 0.4,
    "per_path_counts": {},
    "custom_eval_count": 0,
    "hyperparameters": {
      "embed_size": 1024,
      "block_size": 1024,
      "batch_size": 16,
      "num_epochs": 3,
      "learning_rate": 0.001,
      "kgram_k": 3,
      "kgram_chunk_size": 1
    },
    "model_metrics": {
      "lstm_seq": {
        "epoch_losses": [
          3.458551254272461,
          2.0764169692993164,
          1.7910028493404389
        ],
        "final_loss": 1.7910028493404389,
        "total_steps": 150,
        "elapsed": 19117.04547405243,
        "eval_losses": [
          3.283450173377991,
          2.806197389125824,
          2.58912344121933
        ],
        "final_eval_loss": 2.58912344121933
      },
      "transformer_gptoss": {
        "epoch_losses": [
          3.004897155761719,
          2.09777037858963,
          1.8477948236465453
        ],
        "final_loss": 1.8477948236465453,
        "total_steps": 150,
        "elapsed": 38006.32237696648,
        "eval_losses": [
          2.9567834720611574,
          2.631707974910736,
          2.4469068412780763
        ],
        "final_eval_loss": 2.4469068412780763
      }
    },
    "prompt": "Once upon a"
  },
  {
    "timestamp": 1763311048.403714,
    "tinystories_sequences": 16000,
    "custom_sequences": 0,
    "base_sequences": 0,
    "total_sequences": 0,
    "tinystories_weight": 0.7,
    "limit_custom_examples": null,
    "test_split_ratio": 0.4,
    "per_path_counts": {},
    "custom_eval_count": 0,
    "hyperparameters": {
      "embed_size": 1024,
      "block_size": 1024,
      "batch_size": 16,
      "num_epochs": 3,
      "learning_rate": 0.001,
      "kgram_k": 3,
      "kgram_chunk_size": 1
    },
    "model_metrics": {
      "lstm_seq": {
        "epoch_losses": [
          3.238821094036102,
          2.1962216830253602,
          1.9233966445922852
        ],
        "final_loss": 1.9233966445922852,
        "total_steps": 150,
        "elapsed": 18355.939187049866,
        "eval_losses": [
          3.273623055458069,
          2.802421624660492,
          2.592787181377411
        ],
        "final_eval_loss": 2.592787181377411
      },
      "transformer_gptoss": {
        "epoch_losses": [
          3.1212615823745726,
          1.960472388267517,
          1.7937687730789185
        ],
        "final_loss": 1.7937687730789185,
        "total_steps": 150,
        "elapsed": 39053.48511123657,
        "eval_losses": [
          2.9556418561935427,
          2.659810359477997,
          2.4773029279708862
        ],
        "final_eval_loss": 2.4773029279708862
      }
    },
    "prompt": "Once upon a"
  }
]