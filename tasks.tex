\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{enumitem}
\usepackage{amsmath}

\title{Core Task Progress Notes}
\author{Team: \underline{\hspace{2in}}}
\date{\today}

\begin{document}
\maketitle

\section*{Sanity-Check Run}
\begin{itemize}[leftmargin=*]
  \item \textbf{Command.} \texttt{OMP\_NUM\_THREADS=1 python3 pico-llm.py --tinystories\_weight 0.0 --input\_files 3seqs.txt --block\_size 32 --max\_steps\_per\_epoch 1 --kgram\_k 2 --embed\_size 64 --device\_id cpu}
  \item \textbf{Environment tweaks.} Installed the missing \texttt{tiktoken} package and pinned OpenMP threads to avoid shared-memory warnings in the sandboxed environment.
  \item \textbf{Outcome.} Training loop completed three abbreviated epochs for the LSTM baseline on \texttt{3seqs.txt}, producing loss values (10.82 $\rightarrow$ 10.80 $\rightarrow$ 10.77) and sample generations at greedy, $p=0.95$, and $p=1.0$ settings. This confirms data loading, tokenization, model forward/backward, and text generation pathways function end-to-end on CPU-only hardware.
\end{itemize}

\section*{Key Observations}
\begin{itemize}[leftmargin=*]
  \item The current nucleus sampling placeholder yields identical outputs across different $p$ values, reinforcing the need to implement true top-$p$ sampling before the presentation.
  \item LSTM outputs trained on the minimal dataset remain nonsensical, which is expected given the toy corpus and tiny training budget; future sanity checks on TinyStories or richer data should improve qualitative quality.
  \item Installing dependencies on the target machine (e.g., \texttt{tiktoken}) should be part of setup instructions to prevent runtime failures during the live demo.
\end{itemize}

\section*{Sample Interview Questions \& Answers}
\begin{enumerate}[leftmargin=*]
  \item \textbf{What did you verify in the sanity-check run, and why did you choose the custom dataset?}\\
        We confirmed that the default training loop, tokenizer, and generation routines run without crashing on CPU. The \texttt{3seqs.txt} data ships with the repo, so it avoids the network dependency of downloading TinyStories and shortens iteration time.
  \item \textbf{Why did the generated samples look incoherent, and is that a concern?}\\
        The dataset contains synthetic numeric sequences and we limited training to one gradient step per epoch. With minimal data and budget, the LSTM cannot learn meaningful structure; the purpose of this run was functionality, not quality. Larger datasets and more steps will address coherence.
  \item \textbf{You pinned \texttt{OMP\_NUM\_THREADS} to 1---will that hurt performance later?}\\
        For this quick CPU smoke test it eliminated shared-memory errors. On a full training environment we can remove or raise the cap once OpenMP shared-memory permissions are available, restoring multithreaded BLAS performance.
  \item \textbf{What additional checks will you run before the presentation?}\\
        We plan to repeat the sanity check on TinyStories once Transformer and k-gram models are implemented, capture training curves for the required figures, and verify qualitative outputs under true top-$p$ sampling to demonstrate improved diversity.
\end{enumerate}

\section*{KGramMLPSeqModel Design Options}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Baseline One-Hot MLP.}\\
        \textit{Structure:} Flatten the $k$ one-hot context vectors into a $(k \times |V|)$ input and feed through stacked Linear $\rightarrow$ SiLU blocks ending in a Linear layer to logits.\\
        \textit{Pros:} Direct translation of the docstring; simplest to reason about; no extra embeddings needed.\\
        \textit{Cons:} Extremely high input dimensionality (tens of thousands) makes each forward/backward pass slow and memory hungry; scales poorly with larger vocab or $k$.\\
        \textit{Performance expectations:} Works for tiny vocab toy data but may be impractical for full TinyStories unless chunking is aggressive.
  \item \textbf{Token Embedding + MLP (Shared Projection).}\\
        \textit{Structure:} Map each context token through a learned embedding matrix ($|V| \rightarrow d$), concatenate the $k$ embeddings (size $k \times d$), then apply a compact MLP that projects back to vocabulary logits.\\
        \textit{Pros:} Input dimensionality drops dramatically; parameters scale with $d$ instead of $|V|$; embeddings can be reused or initialized from LSTM/Transformer values.\\
        \textit{Cons:} Slightly more complex; must ensure embeddings are trained jointly; loses the “pure” one-hot formulation.\\
        \textit{Performance expectations:} Much faster and fits GPU memory; still expressive enough for k-gram patterns when $d$ is moderate.
  \item \textbf{Embedding + Depthwise 1D Convolution Hybrid.}\\
        \textit{Structure:} Embed tokens as above, stack along temporal dimension, then use 1D convolution(s) over the $k$ context positions followed by a projection to logits.\\
        \textit{Pros:} Weight sharing across positions reduces parameters; convolutional kernels capture local order efficiently; fits well with chunked inference.\\
        \textit{Cons:} Deviates from strict MLP requirement; requires reshaping logic in the provided forward loop; debugging convolutional behavior may be harder.\\
        \textit{Performance expectations:} Efficient on GPU and handles longer $k$ gracefully; offers inductive bias similar to CNNs for n-gram features.
\end{enumerate}

\section*{Action Plan: Implementing KGramMLPSeqModel}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Select architecture.} Choose between one-hot MLP, embedding-based MLP, or convolution hybrid based on desired trade-offs.
  \item \textbf{Define network layers.} Instantiate embedding (if needed), hidden Linear layers with activation, dropout if desired, and final Linear layer to $|V|$ logits.
  \item \textbf{Integrate into forward loop.} Replace the \texttt{self.net = None} stub and ensure tensors are moved to the correct device, handling chunked processing without altering the provided outer loops.
  \item \textbf{Add safety checks.} Validate input shapes, raise informative errors if \texttt{self.net} is misconfigured, and confirm gradients flow through embeddings.
  \item \textbf{Test in notebook.} Run the reduced configuration from the sanity check, compare loss trajectories, and profile runtime to confirm expected improvements.
\end{enumerate}

\section*{Forward Pass Rationale}
\begin{itemize}[leftmargin=*]
  \item \textbf{Responsibility.} The \texttt{forward()} method ingests a tensor of token IDs with shape (sequence length, batch size), walks through the sequence in micro-batches, extracts the preceding $k$ tokens for each position, pads when history is short, one-hot encodes that context, and ships the flattened vector through the chosen sub-network to obtain logits.
  \item \textbf{Chunking.} Iterating in blocks of size \texttt{chunk\_size} lowers peak memory usage, making the naive Python loop feasible even for long sequences.
  \item \textbf{Why it matters.} Without this logic, the k-gram model would never transform context into next-token distributions—no logits means no loss, no gradients, and no generation. This function therefore defines the core behavior we train and evaluate.
  \item \textbf{Postconditions.} Concatenating the outputs from each chunk restores the expected $(\text{seq\_len}, \text{batch}, |V|)$ tensor that downstream code (loss, sampling) depends on.
\end{itemize}

\section*{Action Plan: Implementing Nucleus Sampling}
\begin{enumerate}[leftmargin=*]
  \item \textbf{Normalize logits.} Convert logits to probabilities via \texttt{softmax} to obtain a stable distribution over the vocabulary.
  \item \textbf{Rank tokens.} Sort probabilities in descending order while keeping token indices to identify the highest-mass prefix.
  \item \textbf{Cumulative cutoff.} Compute the running probability mass and retain the smallest slice whose sum exceeds the user-specified $p$ threshold.
  \item \textbf{Renormalize.} Scale the retained probabilities so they sum to one, ensuring a valid categorical distribution.
  \item \textbf{Sample token.} Draw the next token from the truncated distribution (fall back to the full distribution if $p$ never triggers).
  \item \textbf{Surface API.} Expose the sampler through \texttt{generate\_text} so notebook experiments can vary $p$ without extra wiring.
\end{enumerate}

\section*{Nucleus Sampling Observations}
\begin{itemize}[leftmargin=*]
  \item \textbf{Setup.} Used the quick LSTM sanity-check run (one optimization step on the synthetic corpus) and generated 20 new tokens from the prompt ``Once upon a'' with different top-$p$ thresholds.
  \item \textbf{Greedy ($p=\text{None}$).} \textit{``Once upon a office sofa Iz ACA King investigate likeness ancestorRegarding speaker dive Dum wavesMagikarp Gleaming Authorization Asset hamHor Clinton''}
  \item \textbf{Top-$p=0.8$.} \textit{``Once upon a retroald Continuous Istanbul '/ Issa kids recourse fa Gly EMP (\$) Dig ProxybpJu ceilings RailwayiversityIto''}
  \item \textbf{Top-$p=0.95$.} \textit{``Once upon a Saulfocus340headed Dietaryindividual ideologicallyCW intendederen contributMA tours ML Contribut Nottingham European CHRIST Readers dere''}
  \item \textbf{Top-$p=1.0$.} \textit{``Once upon a Jordan speciesotomy started Cousoccupied Shootifestyle DRMBoot Colorsinav Sit Actionuren dunk now 29 rarity Nguyen''}
  \item \textbf{Takeaway.} Higher $p$ values admit a larger tail of low-probability tokens, yielding more diverse (but noisier) completions. Greedy decoding sticks to the modal choice and repeats motifs, while $p=1.0$ allows maximum randomness.
\end{itemize}

\section*{Optional Task: Custom Training Data}
\begin{itemize}[leftmargin=*]
  \item \textbf{Dataset.} Authored five corpora of hand-written micro-stories in \texttt{data/custom\_corpus/field\_notes.txt}, \texttt{mini\_stories.txt}, \texttt{observatory\_journal.txt}, \texttt{city\_archive.txt}, and the new \texttt{harbor\_diary.txt}. Combined, the directory now holds 182 unique lines, tripling the original custom pool with metro, nocturnal, aerial, and harbor vignettes.
  \item \textbf{Loader Updates.} Added \texttt{--input\_dir} (recursive \texttt{*.txt} discovery) and \texttt{--limit\_custom\_examples} flags so we can point the trainer at an entire folder or trim the number of custom lines for quick tests.
  \item \textbf{Usage.} Example command: \texttt{OMP\_NUM\_THREADS=1 python3 pico-llm.py --tinystories\_weight 0.25 --input\_dir data/custom\_corpus --block\_size 64 --max\_steps\_per\_epoch 1 --enable\_transformer\_variants gptoss}.
  \item \textbf{Result Snapshot.} A quick smoke test with \texttt{--tinystories\_weight 0.25} + the custom corpus loads 182 additional sequences and trains without errors. Even with the one-step-per-epoch cap the models start echoing the new terms (“drone”, “lighthouse”, “meteorite”) instead of the numeric toy lines, confirming that the loader now ingests richer data sources.
  \item \textbf{Loss Sweep.} Recorded short runs at four corpus sizes (3, 63, 123, 185 sequences) with \texttt{--custom\_sweep\_log artifacts/custom\_data\_sweep.json}. The notebook plots the final epoch loss for LSTM and GPT-oss versus corpus size, showing monotonic loss reduction for LSTM and a gentler decline for GPT-oss as more custom data is added.
  \item \textbf{Training Budget Sweep.} Logged fixed-corpus runs with increasing steps per epoch (1, 3, 6) under a 60/40 train/test split. Stored metrics in \texttt{artifacts/training\_budget\_sweep.json}; the notebook visualises how additional gradient steps lower both train and eval losses, clarifying that the earlier loss rise was due to under-training.
  \item \textbf{Next Steps.} Curate a larger custom directory (possibly user transcripts or domain notes) and run longer schedules while logging per-source sampling weights. Compose qualitative comparisons between TinyStories-only and blended corpora for the presentation.
\end{itemize}

\section*{Performance Comparison Strategy}
\begin{itemize}[leftmargin=*]
  \item \textbf{Feasibility.} It is practical to compare all three architectures visually: log training loss and wall-clock time per step for each variant while running identical hyperparameters, then plot the metrics using \texttt{matplotlib} inside the standalone notebook. GPU execution is optional but accelerates experimentation.
  \item \textbf{Instrumentation plan.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
      \item Wrap the training loop to record (loss, step, elapsed seconds) tuples for each KGramMLPSeqModel option via separate helper functions.
      \item Store results in a shared list or \texttt{pandas} DataFrame, noting architecture label and configuration (e.g., embedding dimension).
      \item After each run, generate line plots for loss vs. step and bar charts for tokens/sec to highlight efficiency differences.
    \end{itemize}
  \item \textbf{Caveats.} Full TinyStories runs may be slow in Colab CPU mode; expect to subsample the dataset or limit training steps for a fair yet tractable comparison. Ensure random seeds are fixed for reproducibility.
\end{itemize}

\section*{Overfitting Exploration Plan}
\begin{itemize}[leftmargin=*]
  \item \textbf{Goal.} Demonstrate and study overfitting by training on a deliberately small subset of the custom corpus while monitoring both training and testing loss.
  \item \textbf{Subset Selection.} Sample 20--30 sequences from the custom directory, maintain a 60/40 train/test split so the held-out portion remains informative.
  \item \textbf{Training Schedule.} Run extended schedules (e.g., up to 100 gradient steps) with periodic checkpoints. Capture training/testing loss pairs as well as qualitative generations to observe memorisation.
  \item \textbf{Architectures.} Repeat the procedure for both LSTM and GPT-oss to compare how quickly each model memorises the small dataset.
  \item \textbf{Artifacts.} Log metrics to a new JSON file (e.g., \texttt{artifacts/overfitting\_sweep.json}) so the notebook can add a dedicated plot once runs are complete.
  \item \textbf{Initial Run.} Executed a 30-epoch sweep (batch size 4) on a 43-sequence subset; metrics recorded in \texttt{artifacts/overfitting\_sweep.json} now feed the notebook plot highlighting the divergence between training and testing losses.
\end{itemize}

\section*{Optional Task: General Hyperparameter Study (Plan)}
\begin{itemize}[leftmargin=*]
  \item \textbf{Objective.} Systematically explore non-learned settings (embedding size, block size, number of LSTM layers, Transformer heads/blocks, learning rate, batch size) to see how they affect convergence speed, loss, and sample quality.
  \item \textbf{Experimental Grid.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
      \item \textbf{Scope.} Focus on a manageable subset: e.g., embedding sizes $\{256, 512, 1024\}$, block sizes $\{32, 64, 128\}$, LSTM layers $\{1, 2\}$, Transformer heads $\{4, 8\}$, learning rates $\{5{\\times}10^{-4}, 1{\\times}10^{-3}\}$.
      \item \textbf{Models.} Run both LSTM and GPT-oss for each hyperparameter combo.
      \item \textbf{Budget.} Fix a moderate training budget (e.g., 10 epochs, batch size 8) to keep runtimes tractable while still reflecting differences.
    \end{itemize}
  \item \textbf{Metrics.} Log training/testing loss, tokens-per-second, and representative generations. Store results in \texttt{artifacts/hparam\_sweep.json} for notebook visualisation (e.g., heatmaps or spider plots).
  \item \textbf{Notebook Integration.} Add a “General Hyperparameter Study” section that pulls the JSON results and highlights the best-performing configurations along with trade-offs (quality vs. speed).
  \item \textbf{Deliverables.} Produce plots comparing loss vs. embedding size, loss vs. learning rate, etc., summarise key findings, and note recommendations for the final presentation.
  \item \textbf{Initial Runs.} Logged three configurations (\texttt{emb256/block32/lr5e-4}, \texttt{emb512/block64/lr1e-3}, \texttt{emb1024/block128/lr2e-3}) to \texttt{artifacts/hparam\_sweep.json}; the notebook now visualises their training/testing losses side by side.
\end{itemize}

\section*{Empirical Results (Synthetic Corpus)}
\begin{itemize}[leftmargin=*]
  \item \textbf{Setup.} CPU only; k=2, embedding size 64, two inner MLP layers, Adam with learning rate $1\\times 10^{-3}$, 20 mini-batches per variant (batch size 32) over the \texttt{3seqs.txt} dataset.
  \item \textbf{Metrics.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
      \item One-hot MLP: average loss $\\approx 10.63$, throughput $\\approx 17.6$ tokens/s, runtime $\\approx 1161$ seconds.
      \item Embedding MLP: average loss $\\approx 9.49$, throughput $\\approx 42.3$ tokens/s, runtime $\\approx 485$ seconds.
      \item Conv Hybrid: average loss $\\approx 10.23$, throughput $\\approx 32.3$ tokens/s, runtime $\\approx 634$ seconds.
    \end{itemize}
  \item \textbf{Decision.} The embedding-based MLP achieves the best trade-off (lowest loss and highest throughput), so we will proceed with this architecture for the full implementation and upcoming experiments.
\end{itemize}

\section*{TransformerModel Implementation Outline}
\begin{enumerate}[leftmargin=*]
    \item \textbf{Surface Configuration.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Expose \texttt{d\_model}, \texttt{n\_heads}, \texttt{n\_blocks} (capped at ten), feed-forward width multiplier, dropout rates, and rotary-frequency toggles so we can sweep sizes without rewriting layers.
        \item Share a \texttt{transformer\_variant} flag to map onto the presets listed below (GPT2, GPT-oss, Llama3-lite, minGPT).
    \end{itemize}
    \item \textbf{Token and Positional Embeddings.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Reuse the shared token embedding matrix for the output projection (weight tying).
        \item Implement learned positional embeddings by default; include a rotary positional embedding helper so GPT-oss and Llama-style presets can opt in.
    \end{itemize}
    \item \textbf{RMSNorm and Block Skeleton.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Finish the RMSNorm stub: compute the mean square per hidden vector, scale by \texttt{weight}, and broadcast the epsilon guard.
        \item Each Transformer block follows Pre-Norm layout: \texttt{RMSNorm} $\rightarrow$ Multi-Head Attention + residual $\rightarrow$ \texttt{RMSNorm} $\rightarrow$ Feed-Forward + residual.
    \end{itemize}
    \item \textbf{Multi-Head Attention.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Compute query, key, value tensors via shared Linear projections; reshape to \texttt{(seq, batch, heads, head\_dim)}.
        \item Apply rotary or additive position bias if enabled, mask with causal upper-triangular mask, scale by $\frac{1}{\sqrt{\text{head\_dim}}}$, and reuse the cache-friendly loop already provided in the stub comments.
    \end{itemize}
    \item \textbf{Feed-Forward Network.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Default to GELU two-layer MLP with width multiplier 4$\times$; allow toggling to SwiGLU for the Llama3-lite preset.
        \item Insert dropout hooks that become no-ops during evaluation to match the starter code style.
    \end{itemize}
    \item \textbf{Forward Pass and Output Head.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Accept input as \texttt{(seq, batch)}, emit logits \texttt{(seq, batch, vocab)}.
        \item Tie the embedding weights for the output linear layer unless a variant explicitly disables sharing.
    \end{itemize}
    \item \textbf{Initialization and Guard Rails.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Adopt variant-specific initializers (GPT2 uses scaled normal, Llama-style uses Xavier uniform).
        \item Assert \texttt{n\_heads} divides \texttt{d\_model} and \texttt{n\_blocks} $\leq 10$ to keep runs within the assignment budget.
    \end{itemize}
    \item \textbf{Testing Hooks.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Add unit checks that random inputs produce finite logits and that the KV-cache path matches the dense attention for small sequences.
        \item Benchmark the synthetic dataset with three quick batches to record loss and throughput per variant.
    \end{itemize}
\end{enumerate}

\section*{Transformer Variants and Feasibility}
\begin{itemize}[leftmargin=*]
    \item \textbf{minGPT-inspired (Feasible).}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Architecture: \texttt{d\_model} 384, \texttt{n\_heads} 6, \texttt{n\_blocks} 6, GELU feed-forward width 1536, learned positional embeddings, tied output weights.
        \item Implementation status: fully achievable with the outline above; requires no external dependencies.
        \item Metrics plan: reuse the synthetic benchmark harness (20 batches, batch size 32) and report loss/tokens-per-second next to the KGram results.
    \end{itemize}
    \item \textbf{GPT2-small style (Feasible with scaling).}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Architecture: adapt GPT-2 small to assignment limits by using \texttt{d\_model} 768, \texttt{n\_heads} 12, \texttt{n\_blocks} 8 (reduced from 12 to stay $\leq 10$), learned absolute positions, MLP width 3072, GELU activation, tied output head.
        \item Implementation status: feasible once RMSNorm is replaced with LayerNorm for this preset or we supply a LayerNorm toggle; compute cost is higher but manageable on the synthetic corpus.
        \item Metrics plan: same synthetic harness; for TinyStories we will downscale batch size to keep runtimes under the cap.
    \end{itemize}
    \item \textbf{GPT-oss (Feasible, requires rotary).}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Architecture: \texttt{d\_model} 512, \texttt{n\_heads} 8, \texttt{n\_blocks} 8, rotary positional embeddings, SwiGLU feed-forward width 2048, untied output head per reference design.
        \item Implementation status: feasible after we add a light-weight rotary embedding helper; no external kernels needed.
        \item Metrics plan: record synthetic benchmark figures and optionally a short TinyStories run (max steps per epoch $=1$) to stay within time limits.
    \end{itemize}
    \item \textbf{Llama3-lite (Partially feasible).}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item Architecture: distilled from the published spec with \texttt{d\_model} 1024, \texttt{n\_heads} 16, \texttt{n\_blocks} 8, rotary positions, SwiGLU feed-forward width 8192, RMSNorm everywhere, untied output head.
        \item Implementation status: core building blocks (rotary, SwiGLU, RMSNorm, attention) fit our codebase; full Llama3 features (grouped-query attention, sliding window) exceed current scope. We propose implementing a simplified variant without the extra attention optimisations.
        \item Metrics plan: we can capture synthetic benchmark numbers; collecting full TinyStories metrics would demand GPU time beyond the assignment budget, so we will document only CPU synthetic runs unless instructed otherwise.
    \end{itemize}
    \item \textbf{Performance Collection Summary.}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item All small-batch synthetic metrics are attainable once the Transformer is implemented.
        \item Full-scale TinyStories training for GPT2-small or Llama3-lite is out of scope without extensive runtime; we will note this limitation in results to keep expectations clear.
    \end{itemize}
    \item \textbf{Recorded Metrics (Synthetic Corpus, 3 batches, batch size 16).}
    \begin{itemize}[leftmargin=1.5em,label=\textbf{--}]
        \item minGPT-style: average loss $234.14$, throughput $26.33$ tokens/s, elapsed $0.57$ s.
        \item GPT2-small (trimmed to 8 blocks): average loss $360.30$, throughput $9.35$ tokens/s, elapsed $1.60$ s.
        \item GPT-oss (default): average loss $10.93$, throughput $12.36$ tokens/s, elapsed $1.21$ s.
        \item Benchmarks are gated behind \texttt{--collect\_transformer\_metrics}; the numbers above match the \texttt{RECORDED\_TRANSFORMER\_BENCHMARK} entries in \texttt{pico-llm.py}.
    \end{itemize}
\end{itemize}

\end{document}

