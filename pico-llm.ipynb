{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pico-LLM Standalone Notebook\n",
        "\n",
        "This notebook mirrors the course starter code but runs entirely within Colab (or any Jupyter runtime) without referencing external `.py` files. Modify the cells directly as you implement the core tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Use\n",
        "- Execute the dependency cell (once per runtime) to install `tiktoken` and `datasets`.\n",
        "- Run the subsequent cells in order to define data utilities, model classes, and training helpers.\n",
        "- Adjust the configuration dictionary near the bottom to tweak hyperparameters or switch between TinyStories and synthetic data.\n",
        "- Rerun individual cells as you implement missing pieces (e.g., k-gram MLP, nucleus sampling, Transformer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tiktoken datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mengziyue/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/mengziyue/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA not detected; defaulting to CPU.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "except ImportError as exc:  # safety if pip install skipped\n",
        "    raise RuntimeError(\"Please run the dependency installation cell first.\") from exc\n",
        "\n",
        "try:\n",
        "    import tiktoken\n",
        "except ImportError as exc:\n",
        "    raise RuntimeError(\"Please run the dependency installation cell first.\") from exc\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"CUDA available:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"CUDA not detected; defaulting to CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENC = tiktoken.get_encoding(\"gpt2\")\n",
        "VOCAB_SIZE = ENC.n_vocab\n",
        "\n",
        "\n",
        "def ensure_sample_dataset(path: Path = Path(\"3seqs.txt\"), repeats: int = 1111) -> Path:\n",
        "    \"\"\"Create a tiny numeric dataset if one is not already present.\"\"\"\n",
        "    if path.exists():\n",
        "        return path\n",
        "    with path.open(\"w\") as fp:\n",
        "        for _ in range(repeats):\n",
        "            fp.write(\"0 1 2 3 4\\n\")\n",
        "            fp.write(\"4 3 2 1 0\\n\")\n",
        "            fp.write(\"1 3 5 7 9\\n\")\n",
        "    return path\n",
        "\n",
        "\n",
        "class MixedSequenceDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Match the behavior of the starter code's dataset mixer.\"\"\"\n",
        "\n",
        "    def __init__(self, tinystories_seqs, other_seqs, p_tiny: float):\n",
        "        super().__init__()\n",
        "        self.tinystories_seqs = tinystories_seqs\n",
        "        self.other_seqs = other_seqs\n",
        "        self.p_tiny = p_tiny\n",
        "        self.has_tinystories = len(tinystories_seqs) > 0\n",
        "        self.has_other = len(other_seqs) > 0\n",
        "        self.total_length = len(tinystories_seqs) + len(other_seqs)\n",
        "        if self.total_length == 0:\n",
        "            raise ValueError(\"No data available in TinyStories or custom sources.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx unused due to random sampling logic\n",
        "        r = random.random()\n",
        "        if self.has_tinystories and self.has_other:\n",
        "            if r < self.p_tiny:\n",
        "                seq = random.choice(self.tinystories_seqs)\n",
        "            else:\n",
        "                seq = random.choice(self.other_seqs)\n",
        "        elif self.has_tinystories:\n",
        "            seq = random.choice(self.tinystories_seqs)\n",
        "        else:\n",
        "            seq = random.choice(self.other_seqs)\n",
        "        return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "\n",
        "def seq_collate_fn(batch):\n",
        "    max_len = max(len(seq) for seq in batch)\n",
        "    batch_size = len(batch)\n",
        "    padded = torch.zeros(max_len, batch_size, dtype=torch.long)\n",
        "    for i, seq in enumerate(batch):\n",
        "        seq_len = seq.size(0)\n",
        "        padded[:seq_len, i] = seq\n",
        "    return padded\n",
        "\n",
        "\n",
        "def load_sequences(config: Dict) -> Tuple[MixedSequenceDataset, List[int], List[int]]:\n",
        "    tinystories_seqs: List[List[int]] = []\n",
        "    other_seqs: List[List[int]] = []\n",
        "    block_size = config[\"block_size\"]\n",
        "    p_tiny = config[\"tinystories_weight\"]\n",
        "\n",
        "    if p_tiny > 0.0:\n",
        "        print(f\"Loading TinyStories subset (size={config['train_subset_size']})...\")\n",
        "        dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "        dataset = dataset.select(range(config[\"train_subset_size\"]))\n",
        "        for sample in dataset:\n",
        "            tokens = ENC.encode(sample[\"text\"])[:block_size]\n",
        "            if tokens:\n",
        "                tinystories_seqs.append(tokens)\n",
        "        print(\"TinyStories sequences:\", len(tinystories_seqs))\n",
        "    else:\n",
        "        print(\"TinyStories weight=0 => skipping TinyStories.\")\n",
        "\n",
        "    if config.get(\"use_synthetic\", True):\n",
        "        ensure_sample_dataset()\n",
        "        with Path(\"3seqs.txt\").open() as fp:\n",
        "            for line in fp:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                tokens = ENC.encode(line)[:block_size]\n",
        "                if tokens:\n",
        "                    other_seqs.append(tokens)\n",
        "        print(\"Synthetic sequences:\", len(other_seqs))\n",
        "\n",
        "    dataset = MixedSequenceDataset(tinystories_seqs, other_seqs, p_tiny)\n",
        "    return dataset, tinystories_seqs, other_seqs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_next_token_loss(logits: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
        "    seq_len, batch_size, vocab_size = logits.shape\n",
        "    if seq_len < 2:\n",
        "        return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
        "    preds = logits[:-1]\n",
        "    gold = tokens[1:]\n",
        "    preds = preds.reshape(-1, vocab_size)\n",
        "    gold = gold.reshape(-1)\n",
        "    return F.cross_entropy(preds, gold)\n",
        "\n",
        "\n",
        "class KGramMLPSeqModel(nn.Module):\n",
        "    \"\"\"Stub matching the course starter code. Fill in self.net to complete the task.\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, k=3, embed_size=1024, num_inner_layers=1, chunk_size=1):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.num_inner_layers = num_inner_layers\n",
        "        self.chunk_size = chunk_size\n",
        "        # TODO: implement architecture\n",
        "        self.net = None\n",
        "\n",
        "    def forward(self, tokens_seq):\n",
        "        seq_len, batch_size = tokens_seq.shape\n",
        "        outputs = []\n",
        "        start = 0\n",
        "        while start < seq_len:\n",
        "            end = min(start + self.chunk_size, seq_len)\n",
        "            block_outputs = []\n",
        "            for t in range(start, end):\n",
        "                batch_logits = []\n",
        "                for b in range(batch_size):\n",
        "                    if t < self.k:\n",
        "                        needed = self.k - t\n",
        "                        context_ids = [0] * needed + tokens_seq[:t, b].tolist()\n",
        "                    else:\n",
        "                        context_ids = tokens_seq[t - self.k:t, b].tolist()\n",
        "                    context_oh = F.one_hot(\n",
        "                        torch.tensor(context_ids, dtype=torch.long, device=tokens_seq.device),\n",
        "                        num_classes=self.vocab_size,\n",
        "                    )\n",
        "                    context_flat = context_oh.flatten().float().unsqueeze(0)\n",
        "                    if self.net is None:\n",
        "                        raise NotImplementedError(\"KGramMLPSeqModel.net is not defined yet.\")\n",
        "                    logits_b = self.net(context_flat)\n",
        "                    batch_logits.append(logits_b)\n",
        "                block_outputs.append(torch.cat(batch_logits, dim=0).unsqueeze(0))\n",
        "            block_outputs = torch.cat(block_outputs, dim=0)\n",
        "            outputs.append(block_outputs)\n",
        "            start = end\n",
        "        outputs = torch.cat(outputs, dim=0)\n",
        "        return outputs\n",
        "\n",
        "\n",
        "class LSTMSeqModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=1024, hidden_size=1024):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=False)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, tokens_seq):\n",
        "        emb = self.embedding(tokens_seq)\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(emb)\n",
        "        logits = self.linear(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        # TODO: implement per-assignment\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError(\"RMSNorm is currently a stub.\")\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size=VOCAB_SIZE, d_model=1024, n_heads=2, n_blocks=4):\n",
        "        super().__init__()\n",
        "        # TODO: implement Transformer (token embeddings, blocks, etc.)\n",
        "        raise NotImplementedError(\"TransformerModel is currently a stub.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nucleus_sampling(logits: torch.Tensor, p: float = 0.95) -> int:\n",
        "    # TODO: replace with true top-p sampling. Currently greedy for parity with starter code.\n",
        "    return torch.argmax(logits).item()\n",
        "\n",
        "\n",
        "def monosemantic_analysis_for_token(token_id, model, enc, device=\"cpu\", top_n=5):\n",
        "    # Placeholder returning empty list to match starter behavior.\n",
        "    return []\n",
        "\n",
        "\n",
        "def generate_text(\n",
        "    model,\n",
        "    enc,\n",
        "    init_text,\n",
        "    max_new_tokens: int = 20,\n",
        "    device: str = \"cpu\",\n",
        "    top_p: Optional[float] = None,\n",
        "    monosemantic_info=None,\n",
        "    do_monosemantic: bool = False,\n",
        "):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        context_tokens = enc.encode(init_text)\n",
        "        annotation_list = []\n",
        "        for step_i in range(max_new_tokens):\n",
        "            seq_tensor = torch.tensor(context_tokens, dtype=torch.long, device=device).unsqueeze(1)\n",
        "            logits_seq = model(seq_tensor)\n",
        "            next_logits = logits_seq[-1, 0, :]\n",
        "            if top_p is None:\n",
        "                chosen_token = torch.argmax(next_logits).item()\n",
        "            else:\n",
        "                chosen_token = nucleus_sampling(next_logits, p=top_p)\n",
        "            context_tokens.append(chosen_token)\n",
        "            if do_monosemantic and monosemantic_info is not None:\n",
        "                neighbors = monosemantic_analysis_for_token(\n",
        "                    chosen_token, model, monosemantic_info, enc, device=device, top_n=5\n",
        "                )\n",
        "                annotation_list.append((chosen_token, neighbors))\n",
        "            else:\n",
        "                annotation_list.append((chosen_token, []))\n",
        "    model.train(was_training)\n",
        "    final_text = enc.decode(context_tokens)\n",
        "    prefix_text = enc.decode(context_tokens[:-max_new_tokens])\n",
        "    annotated_strs = [prefix_text]\n",
        "    for (tid, neighs) in annotation_list:\n",
        "        token_str = enc.decode([tid])\n",
        "        if neighs:\n",
        "            neighbor_strs = [f\"{enc.decode([x[1]])}\" for x in neighs]\n",
        "            annotated = f\"{token_str}[NN={neighbor_strs}]\"\n",
        "        else:\n",
        "            annotated = token_str\n",
        "        annotated_strs.append(annotated)\n",
        "    annotated_text = \"\".join(annotated_strs)\n",
        "    return final_text, annotated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_model(\n",
        "    model: nn.Module,\n",
        "    loader: torch.utils.data.DataLoader,\n",
        "    epochs: int,\n",
        "    model_name: str,\n",
        "    device: torch.device,\n",
        "    lr: float = 1e-3,\n",
        "    log_steps: int = 100,\n",
        "    sample_interval: float = 30.0,\n",
        "    max_steps_per_epoch: Optional[int] = None,\n",
        "    prompt: str = \"Once upon a\",\n",
        "):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    start_time = time.time()\n",
        "    next_sample_time = start_time\n",
        "    global_step = 0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        partial_loss = 0.0\n",
        "        partial_count = 0\n",
        "        step_in_epoch = 0\n",
        "        for batch_idx, batch_tokens in enumerate(loader, start=1):\n",
        "            step_in_epoch += 1\n",
        "            global_step += 1\n",
        "            batch_tokens = batch_tokens.to(device)\n",
        "            logits = model(batch_tokens)\n",
        "            loss = compute_next_token_loss(logits, batch_tokens)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            partial_loss += loss.item()\n",
        "            partial_count += 1\n",
        "            if batch_idx % log_steps == 0:\n",
        "                avg_part_loss = partial_loss / partial_count\n",
        "                print(\n",
        "                    f\"[{model_name}] Epoch {epoch}/{epochs}, Step {batch_idx}/{len(loader)}\"\n",
        "                    f\" (global={global_step}) Partial Avg Loss: {avg_part_loss:.4f}\"\n",
        "                )\n",
        "                partial_loss = 0.0\n",
        "                partial_count = 0\n",
        "            current_time = time.time()\n",
        "            if current_time >= next_sample_time:\n",
        "                text_greedy, _ = generate_text(model, ENC, prompt, max_new_tokens=20, device=str(device))\n",
        "                print(f\"[{model_name}] Sample (greedy): {text_greedy}\")\n",
        "                next_sample_time = current_time + sample_interval\n",
        "            if max_steps_per_epoch is not None and step_in_epoch >= max_steps_per_epoch:\n",
        "                print(\n",
        "                    f\"[{model_name}] Reached max_steps_per_epoch={max_steps_per_epoch}, \"\n",
        "                    f\"ending epoch {epoch} early.\"\n",
        "                )\n",
        "                break\n",
        "        avg_loss = total_loss / step_in_epoch\n",
        "        print(f\"[{model_name}] *** End of Epoch {epoch} *** Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "def run_experiment(config: Dict):\n",
        "    dataset, tinystories_seqs, other_seqs = load_sequences(config)\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=seq_collate_fn,\n",
        "    )\n",
        "    device = DEVICE if config[\"device_id\"] == \"auto\" else torch.device(config[\"device_id\"])\n",
        "    print(\n",
        "        \"Using device:\",\n",
        "        device,\n",
        "        \"block_size=\", config[\"block_size\"],\n",
        "        \"kgram_k=\", config[\"kgram_k\"],\n",
        "        \"chunk_size=\", config[\"kgram_chunk_size\"],\n",
        "        \"embed_size=\", config[\"embed_size\"],\n",
        "    )\n",
        "    models: Dict[str, nn.Module] = {}\n",
        "    if config.get(\"enable_kgram\", False):\n",
        "        variant = config.get(\"kgram_variant\", \"embedding\")\n",
        "        allow_alt = config.get(\"allow_alt_kgram\", False)\n",
        "        if variant != \"embedding\" and not allow_alt:\n",
        "            raise ValueError(\n",
        "                \"Set allow_alt_kgram=True to try the onehot or conv variants; they are gated by default.\"\n",
        "            )\n",
        "        models[\"kgram_mlp_seq\"] = KGramMLPSeqModel(\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            k=config[\"kgram_k\"],\n",
        "            embed_size=config[\"embed_size\"],\n",
        "            num_inner_layers=config[\"num_inner_layers\"],\n",
        "            chunk_size=config[\"kgram_chunk_size\"],\n",
        "            variant=variant,\n",
        "            hidden_dim=config.get(\"kgram_hidden_dim\", 512),\n",
        "            conv_hidden_dim=config.get(\"kgram_conv_hidden_dim\", 512),\n",
        "            allow_alt_variants=allow_alt,\n",
        "        ).to(device)\n",
        "    if config.get(\"enable_lstm\", True):\n",
        "        models[\"lstm_seq\"] = LSTMSeqModel(\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            embed_size=config[\"embed_size\"],\n",
        "            hidden_size=config[\"embed_size\"],\n",
        "        ).to(device)\n",
        "    if config.get(\"enable_transformer\", False):\n",
        "        models[\"transformer\"] = TransformerModel().to(device)\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n=== Training model: {name} ===\")\n",
        "        train_one_model(\n",
        "            model,\n",
        "            loader,\n",
        "            epochs=config[\"epochs\"],\n",
        "            model_name=name,\n",
        "            device=device,\n",
        "            lr=config[\"learning_rate\"],\n",
        "            log_steps=config[\"log_interval_steps\"],\n",
        "            sample_interval=config[\"sample_interval_seconds\"],\n",
        "            max_steps_per_epoch=config[\"max_steps_per_epoch\"],\n",
        "            prompt=config[\"prompt\"],\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            text_greedy, ann_greedy = generate_text(\n",
        "                model, ENC, config[\"prompt\"], max_new_tokens=20, device=str(device), top_p=None\n",
        "            )\n",
        "            text_topp, ann_topp = generate_text(\n",
        "                model, ENC, config[\"prompt\"], max_new_tokens=20, device=str(device), top_p=0.95\n",
        "            )\n",
        "            text_topp1, ann_topp1 = generate_text(\n",
        "                model, ENC, config[\"prompt\"], max_new_tokens=20, device=str(device), top_p=1.0\n",
        "            )\n",
        "        results[name] = {\n",
        "            \"greedy\": (text_greedy, ann_greedy),\n",
        "            \"top_p_0.95\": (text_topp, ann_topp),\n",
        "            \"top_p_1.0\": (text_topp1, ann_topp1),\n",
        "        }\n",
        "        print(f\"[{name}] Final sample (greedy):\\n{text_greedy}\\n\")\n",
        "    print(\"\\n*** Run complete ***\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-gram Variant Benchmark (Optional)\n",
        "Use the cell below to compare the three architectural variants. Non-embedding variants are gated; set `allow_alt` to `True` when explicitly benchmarking them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'config' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 66\u001b[0m\n\u001b[1;32m     56\u001b[0m         metrics\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m     57\u001b[0m             {\n\u001b[1;32m     58\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariant\u001b[39m\u001b[38;5;124m\"\u001b[39m: variant,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m             }\n\u001b[1;32m     63\u001b[0m         )\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n\u001b[0;32m---> 66\u001b[0m variant_metrics \u001b[38;5;241m=\u001b[39m \u001b[43mbenchmark_kgram_variants\u001b[49m\u001b[43m(\u001b[49m\u001b[43mallow_alt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(variant_metrics)\n\u001b[1;32m     69\u001b[0m labels \u001b[38;5;241m=\u001b[39m [m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvariant\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m variant_metrics]\n",
            "Cell \u001b[0;32mIn[7], line 10\u001b[0m, in \u001b[0;36mbenchmark_kgram_variants\u001b[0;34m(variants, allow_alt, max_batches, epochs, batch_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbenchmark_kgram_variants\u001b[39m(variants\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monehot\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconv\u001b[39m\u001b[38;5;124m\"\u001b[39m), allow_alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      4\u001b[0m                              max_batches\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m):\n\u001b[1;32m      5\u001b[0m     ensure_sample_dataset()\n\u001b[1;32m      6\u001b[0m     dataset, _, _ \u001b[38;5;241m=\u001b[39m load_sequences({\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtinystories_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_synthetic\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_subset_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m---> 10\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mconfig\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock_size\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     11\u001b[0m     })\n\u001b[1;32m     12\u001b[0m     loader \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(\n\u001b[1;32m     13\u001b[0m         dataset,\n\u001b[1;32m     14\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m         collate_fn\u001b[38;5;241m=\u001b[39mseq_collate_fn,\n\u001b[1;32m     18\u001b[0m     )\n\u001b[1;32m     19\u001b[0m     device \u001b[38;5;241m=\u001b[39m DEVICE\n",
            "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def benchmark_kgram_variants(\n",
        "    variants=(\"embedding\", \"onehot\", \"conv\"),\n",
        "    allow_alt=False,\n",
        "    max_batches=10,\n",
        "    epochs=1,\n",
        "    batch_size=32,\n",
        "    block_size=32,\n",
        "    kgram_k=2,\n",
        "    embed_size=64,\n",
        "    num_inner_layers=1,\n",
        "    chunk_size=1,\n",
        "    learning_rate=1e-3,\n",
        "    hidden_dim=512,\n",
        "    conv_hidden_dim=512,\n",
        "):\n",
        "    ensure_sample_dataset()\n",
        "    dataset, _, _ = load_sequences(\n",
        "        {\n",
        "            \"tinystories_weight\": 0.0,\n",
        "            \"use_synthetic\": True,\n",
        "            \"train_subset_size\": 1,\n",
        "            \"block_size\": block_size,\n",
        "        }\n",
        "    )\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=seq_collate_fn,\n",
        "    )\n",
        "    device = DEVICE\n",
        "    metrics = []\n",
        "    for variant in variants:\n",
        "        allow_flag = allow_alt if variant != \"embedding\" else False\n",
        "        model = KGramMLPSeqModel(\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            k=kgram_k,\n",
        "            embed_size=embed_size,\n",
        "            num_inner_layers=num_inner_layers,\n",
        "            chunk_size=chunk_size,\n",
        "            variant=variant,\n",
        "            hidden_dim=hidden_dim,\n",
        "            conv_hidden_dim=conv_hidden_dim,\n",
        "            allow_alt_variants=allow_flag,\n",
        "        ).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "        tokens_processed = 0\n",
        "        start = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            for batch_idx, batch_tokens in enumerate(loader):\n",
        "                if batch_idx >= max_batches:\n",
        "                    break\n",
        "                batch_tokens = batch_tokens.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(batch_tokens)\n",
        "                loss = compute_next_token_loss(logits, batch_tokens)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                steps += 1\n",
        "                tokens_processed += batch_tokens.numel()\n",
        "            else:\n",
        "                continue\n",
        "            break\n",
        "        elapsed = time.time() - start\n",
        "        metrics.append(\n",
        "            {\n",
        "                \"variant\": variant,\n",
        "                \"avg_loss\": total_loss / max(steps, 1),\n",
        "                \"tokens_per_sec\": tokens_processed / max(elapsed, 1e-6),\n",
        "                \"elapsed\": elapsed,\n",
        "            }\n",
        "        )\n",
        "    return metrics\n",
        "\n",
        "variant_metrics = benchmark_kgram_variants(allow_alt=True)\n",
        "print(variant_metrics)\n",
        "\n",
        "labels = [m[\"variant\"] for m in variant_metrics]\n",
        "losses = [m[\"avg_loss\"] for m in variant_metrics]\n",
        "throughputs = [m[\"tokens_per_sec\"] for m in variant_metrics]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].bar(labels, losses, color=[\"#4c72b0\", \"#55a868\", \"#c44e52\"])\n",
        "axes[0].set_title(\"Average Loss\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[1].bar(labels, throughputs, color=[\"#4c72b0\", \"#55a868\", \"#c44e52\"])\n",
        "axes[1].set_title(\"Tokens per Second\")\n",
        "axes[1].set_ylabel(\"tokens/s\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recorded Benchmark Results\n",
        "The plot below visualizes the synthetic-corpus experiment comparing the three K-gram variants (20 mini-batches, CPU-only).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "recorded_metrics = [\n",
        "    {\"variant\": \"embedding\", \"avg_loss\": 9.4895, \"tokens_per_sec\": 42.3},\n",
        "    {\"variant\": \"conv\", \"avg_loss\": 10.2317, \"tokens_per_sec\": 32.3},\n",
        "    {\"variant\": \"onehot\", \"avg_loss\": 10.6328, \"tokens_per_sec\": 17.6},\n",
        "]\n",
        "labels = [m[\"variant\"] for m in recorded_metrics]\n",
        "losses = [m[\"avg_loss\"] for m in recorded_metrics]\n",
        "throughputs = [m[\"tokens_per_sec\"] for m in recorded_metrics]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].bar(labels, losses, color=[\"#4c72b0\", \"#dd8452\", \"#55a868\"], alpha=0.8)\n",
        "axes[0].set_title(\"Average Loss (Lower is Better)\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "axes[1].bar(labels, throughputs, color=[\"#4c72b0\", \"#dd8452\", \"#55a868\"], alpha=0.8)\n",
        "axes[1].set_title(\"Throughput (Higher is Better)\")\n",
        "axes[1].set_ylabel(\"Tokens / Second\")\n",
        "axes[1].grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "plt.suptitle(\"K-Gram Variant Comparison (Synthetic Dataset)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "or"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"tinystories_weight\": 0.0,  # set >0 to mix in TinyStories\n",
        "    \"use_synthetic\": True,\n",
        "    \"train_subset_size\": 20000,\n",
        "    \"block_size\": 32,\n",
        "    \"batch_size\": 16,\n",
        "    \"epochs\": 3,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"log_interval_steps\": 100,\n",
        "    \"sample_interval_seconds\": 30,\n",
        "    \"max_steps_per_epoch\": 1,\n",
        "    \"kgram_k\": 2,\n",
        "    \"kgram_chunk_size\": 1,\n",
        "    \"embed_size\": 64,\n",
        "    \"num_inner_layers\": 1,\n",
        "    \"kgram_variant\": \"embedding\",\n",
        "    \"allow_alt_kgram\": False,\n",
        "    \"kgram_hidden_dim\": 512,\n",
        "    \"kgram_conv_hidden_dim\": 512,\n",
        "    \"enable_kgram\": False,\n",
        "    \"enable_lstm\": True,\n",
        "    \"enable_transformer\": False,\n",
        "    \"prompt\": \"Once upon a\",\n",
        "    \"device_id\": \"auto\",  # or \"cpu\" / \"cuda:0\"\n",
        "}\n",
        "\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "results = run_experiment(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- Implement the remaining TODOs directly in this notebook (k-gram MLP, true top-$p$ sampling, RMSNorm, Transformer blocks).\n",
        "- Create additional cells for evaluation figures, loss curves, or attention visualizations.\n",
        "- Save results to Google Drive or another persistent location if you plan to revisit the session later.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
