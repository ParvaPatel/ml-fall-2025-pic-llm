{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pico-LLM Standalone Notebook\n",
        "\n",
        "This notebook mirrors the course starter code but runs entirely within Colab (or any Jupyter runtime) without referencing external `.py` files. Modify the cells directly as you implement the core tasks.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to Use\n",
        "- Execute the dependency cell (once per runtime) to install `tiktoken` and `datasets`.\n",
        "- Run the subsequent cells in order to define data utilities, model classes, and training helpers.\n",
        "- Adjust the configuration dictionary near the bottom to tweak hyperparameters or switch between TinyStories and synthetic data.\n",
        "- Rerun individual cells as you implement missing pieces (e.g., k-gram MLP, nucleus sampling, Transformer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tiktoken datasets\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Imports and Environment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mengziyue/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
            "  warnings.warn(\n",
            "/Users/mengziyue/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CUDA not detected; defaulting to CPU.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "import math\n",
        "from pathlib import Path\n",
        "from typing import Dict, List, Optional, Tuple\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "try:\n",
        "    from datasets import load_dataset\n",
        "except ImportError as exc:  # safety if pip install skipped\n",
        "    raise RuntimeError(\"Please run the dependency installation cell first.\") from exc\n",
        "\n",
        "try:\n",
        "    import tiktoken\n",
        "except ImportError as exc:\n",
        "    raise RuntimeError(\"Please run the dependency installation cell first.\") from exc\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = torch.device(\"cuda\")\n",
        "    print(\"CUDA available:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    DEVICE = torch.device(\"cpu\")\n",
        "    print(\"CUDA not detected; defaulting to CPU.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENC = tiktoken.get_encoding(\"gpt2\")\n",
        "VOCAB_SIZE = ENC.n_vocab\n",
        "\n",
        "\n",
        "def ensure_sample_dataset(path: Path = Path(\"3seqs.txt\"), repeats: int = 1111) -> Path:\n",
        "    \"\"\"Create a tiny numeric dataset if one is not already present.\"\"\"\n",
        "    if path.exists():\n",
        "        return path\n",
        "    with path.open(\"w\") as fp:\n",
        "        for _ in range(repeats):\n",
        "            fp.write(\"0 1 2 3 4\\n\")\n",
        "            fp.write(\"4 3 2 1 0\\n\")\n",
        "            fp.write(\"1 3 5 7 9\\n\")\n",
        "    return path\n",
        "\n",
        "\n",
        "class MixedSequenceDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Match the behavior of the starter code's dataset mixer.\"\"\"\n",
        "\n",
        "    def __init__(self, tinystories_seqs, other_seqs, p_tiny: float):\n",
        "        super().__init__()\n",
        "        self.tinystories_seqs = tinystories_seqs\n",
        "        self.other_seqs = other_seqs\n",
        "        self.p_tiny = p_tiny\n",
        "        self.has_tinystories = len(tinystories_seqs) > 0\n",
        "        self.has_other = len(other_seqs) > 0\n",
        "        self.total_length = len(tinystories_seqs) + len(other_seqs)\n",
        "        if self.total_length == 0:\n",
        "            raise ValueError(\"No data available in TinyStories or custom sources.\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.total_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx unused due to random sampling logic\n",
        "        r = random.random()\n",
        "        if self.has_tinystories and self.has_other:\n",
        "            if r < self.p_tiny:\n",
        "                seq = random.choice(self.tinystories_seqs)\n",
        "            else:\n",
        "                seq = random.choice(self.other_seqs)\n",
        "        elif self.has_tinystories:\n",
        "            seq = random.choice(self.tinystories_seqs)\n",
        "        else:\n",
        "            seq = random.choice(self.other_seqs)\n",
        "        return torch.tensor(seq, dtype=torch.long)\n",
        "\n",
        "\n",
        "def seq_collate_fn(batch):\n",
        "    max_len = max(len(seq) for seq in batch)\n",
        "    batch_size = len(batch)\n",
        "    padded = torch.zeros(max_len, batch_size, dtype=torch.long)\n",
        "    for i, seq in enumerate(batch):\n",
        "        seq_len = seq.size(0)\n",
        "        padded[:seq_len, i] = seq\n",
        "    return padded\n",
        "\n",
        "\n",
        "def load_sequences(config: Dict) -> Tuple[MixedSequenceDataset, List[int], List[int]]:\n",
        "    tinystories_seqs: List[List[int]] = []\n",
        "    other_seqs: List[List[int]] = []\n",
        "    block_size = config[\"block_size\"]\n",
        "    p_tiny = config[\"tinystories_weight\"]\n",
        "\n",
        "    if p_tiny > 0.0:\n",
        "        print(f\"Loading TinyStories subset (size={config['train_subset_size']})...\")\n",
        "        dataset = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
        "        dataset = dataset.select(range(config[\"train_subset_size\"]))\n",
        "        for sample in dataset:\n",
        "            tokens = ENC.encode(sample[\"text\"])[:block_size]\n",
        "            if tokens:\n",
        "                tinystories_seqs.append(tokens)\n",
        "        print(\"TinyStories sequences:\", len(tinystories_seqs))\n",
        "    else:\n",
        "        print(\"TinyStories weight=0 => skipping TinyStories.\")\n",
        "\n",
        "    if config.get(\"use_synthetic\", True):\n",
        "        ensure_sample_dataset()\n",
        "        with Path(\"3seqs.txt\").open() as fp:\n",
        "            for line in fp:\n",
        "                line = line.strip()\n",
        "                if not line:\n",
        "                    continue\n",
        "                tokens = ENC.encode(line)[:block_size]\n",
        "                if tokens:\n",
        "                    other_seqs.append(tokens)\n",
        "        print(\"Synthetic sequences:\", len(other_seqs))\n",
        "\n",
        "    dataset = MixedSequenceDataset(tinystories_seqs, other_seqs, p_tiny)\n",
        "    return dataset, tinystories_seqs, other_seqs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_next_token_loss(logits: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
        "    seq_len, batch_size, vocab_size = logits.shape\n",
        "    if seq_len < 2:\n",
        "        return torch.tensor(0.0, device=logits.device, requires_grad=True)\n",
        "    preds = logits[:-1]\n",
        "    gold = tokens[1:]\n",
        "    preds = preds.reshape(-1, vocab_size)\n",
        "    gold = gold.reshape(-1)\n",
        "    return F.cross_entropy(preds, gold)\n",
        "\n",
        "\n",
        "def build_kgram_onehot_mlp(vocab_size: int, k: int, hidden_dim: int, num_inner_layers: int) -> nn.Sequential:\n",
        "    layers: list[nn.Module] = []\n",
        "    input_dim = k * vocab_size\n",
        "    layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "    layers.append(nn.SiLU())\n",
        "    for _ in range(max(num_inner_layers - 1, 0)):\n",
        "        layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "        layers.append(nn.SiLU())\n",
        "    layers.append(nn.Linear(hidden_dim, vocab_size))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class KGramEmbeddingMLP(nn.Module):\n",
        "    def __init__(self, vocab_size: int, k: int, embed_dim: int, hidden_dim: int, num_inner_layers: int):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        layers: list[nn.Module] = []\n",
        "        input_dim = k * embed_dim\n",
        "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        layers.append(nn.SiLU())\n",
        "        for _ in range(max(num_inner_layers - 1, 0)):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(nn.SiLU())\n",
        "        layers.append(nn.Linear(hidden_dim, vocab_size))\n",
        "        self.mlp = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, context_flat: torch.Tensor) -> torch.Tensor:\n",
        "        batch = context_flat.size(0)\n",
        "        context = context_flat.view(batch, self.k, self.vocab_size)\n",
        "        embedded = torch.matmul(context, self.embed.weight)\n",
        "        x = embedded.reshape(batch, -1)\n",
        "        return self.mlp(x)\n",
        "\n",
        "\n",
        "class KGramConvEmbeddingMLP(nn.Module):\n",
        "    def __init__(self, vocab_size: int, k: int, embed_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
        "        kernel_size = max(1, k)\n",
        "        self.depthwise = nn.Conv1d(embed_dim, embed_dim, kernel_size=kernel_size, groups=embed_dim, bias=False)\n",
        "        self.pointwise = nn.Conv1d(embed_dim, hidden_dim, kernel_size=1)\n",
        "        self.activation = nn.SiLU()\n",
        "        self.head = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, context_flat: torch.Tensor) -> torch.Tensor:\n",
        "        batch = context_flat.size(0)\n",
        "        context = context_flat.view(batch, self.k, self.vocab_size)\n",
        "        embedded = torch.matmul(context, self.embed.weight)\n",
        "        emb = embedded.permute(0, 2, 1)\n",
        "        conv_out = self.pointwise(self.depthwise(emb))\n",
        "        pooled = conv_out.mean(dim=-1)\n",
        "        x = self.activation(pooled)\n",
        "        return self.head(x)\n",
        "\n",
        "\n",
        "class KGramMLPSeqModel(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size,\n",
        "        k=3,\n",
        "        embed_size=1024,\n",
        "        num_inner_layers=1,\n",
        "        chunk_size=1,\n",
        "        variant: str = \"embedding\",\n",
        "        hidden_dim: int = 512,\n",
        "        conv_hidden_dim: int = 512,\n",
        "        allow_alt_variants: bool = False,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.k = k\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        self.num_inner_layers = num_inner_layers\n",
        "        self.chunk_size = chunk_size\n",
        "        variant = variant.lower()\n",
        "        if variant != \"embedding\" and not allow_alt_variants:\n",
        "            raise ValueError(\n",
        "                f\"Variant '{variant}' requires allow_alt_variants=True; defaulting to the embedding MLP.\"\n",
        "            )\n",
        "        if variant == \"onehot\":\n",
        "            self.net = build_kgram_onehot_mlp(vocab_size, k, hidden_dim, num_inner_layers)\n",
        "        elif variant == \"embedding\":\n",
        "            self.net = KGramEmbeddingMLP(vocab_size, k, embed_size, hidden_dim, num_inner_layers)\n",
        "        elif variant == \"conv\":\n",
        "            self.net = KGramConvEmbeddingMLP(vocab_size, k, embed_size, conv_hidden_dim)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown KGramMLPSeqModel variant '{variant}'.\")\n",
        "        self.variant = variant\n",
        "\n",
        "    def forward(self, tokens_seq):\n",
        "        seq_len, batch_size = tokens_seq.shape\n",
        "        outputs = []\n",
        "        start = 0\n",
        "        while start < seq_len:\n",
        "            end = min(start + self.chunk_size, seq_len)\n",
        "            block_outputs = []\n",
        "            for t in range(start, end):\n",
        "                batch_logits = []\n",
        "                for b in range(batch_size):\n",
        "                    if t < self.k:\n",
        "                        needed = self.k - t\n",
        "                        context_ids = [0] * needed + tokens_seq[:t, b].tolist()\n",
        "                    else:\n",
        "                        context_ids = tokens_seq[t - self.k : t, b].tolist()\n",
        "                    context_oh = F.one_hot(\n",
        "                        torch.tensor(context_ids, dtype=torch.long, device=tokens_seq.device),\n",
        "                        num_classes=self.vocab_size,\n",
        "                    )\n",
        "                    context_flat = context_oh.flatten().float().unsqueeze(0)\n",
        "                    logits_b = self.net(context_flat)\n",
        "                    batch_logits.append(logits_b)\n",
        "                block_outputs.append(torch.cat(batch_logits, dim=0).unsqueeze(0))\n",
        "            block_outputs = torch.cat(block_outputs, dim=0)\n",
        "            outputs.append(block_outputs)\n",
        "            start = end\n",
        "        return torch.cat(outputs, dim=0)\n",
        "\n",
        "\n",
        "class LSTMSeqModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=1024, hidden_size=1024):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=False)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, tokens_seq):\n",
        "        emb = self.embedding(tokens_seq)\n",
        "        self.lstm.flatten_parameters()\n",
        "        out, _ = self.lstm(emb)\n",
        "        logits = self.linear(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, dim, eps=1e-5):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(dim))\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, x):\n",
        "        raise NotImplementedError(\"RMSNorm is currently a stub.\")\n",
        "\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, vocab_size=VOCAB_SIZE, d_model=1024, n_heads=2, n_blocks=4):\n",
        "        super().__init__()\n",
        "        raise NotImplementedError(\"TransformerModel is currently a stub.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generation Utilities\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def nucleus_sampling(logits: torch.Tensor, p: float = 0.95) -> int:\n",
        "    # TODO: replace with true top-p sampling. Currently greedy for parity with starter code.\n",
        "    return torch.argmax(logits).item()\n",
        "\n",
        "\n",
        "def monosemantic_analysis_for_token(token_id, model, enc, device=\"cpu\", top_n=5):\n",
        "    # Placeholder returning empty list to match starter behavior.\n",
        "    return []\n",
        "\n",
        "\n",
        "def generate_text(\n",
        "    model,\n",
        "    enc,\n",
        "    init_text,\n",
        "    max_new_tokens: int = 20,\n",
        "    device: str = \"cpu\",\n",
        "    top_p: Optional[float] = None,\n",
        "    monosemantic_info=None,\n",
        "    do_monosemantic: bool = False,\n",
        "):\n",
        "    was_training = model.training\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        context_tokens = enc.encode(init_text)\n",
        "        annotation_list = []\n",
        "        for step_i in range(max_new_tokens):\n",
        "            seq_tensor = torch.tensor(context_tokens, dtype=torch.long, device=device).unsqueeze(1)\n",
        "            logits_seq = model(seq_tensor)\n",
        "            next_logits = logits_seq[-1, 0, :]\n",
        "            if top_p is None:\n",
        "                chosen_token = torch.argmax(next_logits).item()\n",
        "            else:\n",
        "                chosen_token = nucleus_sampling(next_logits, p=top_p)\n",
        "            context_tokens.append(chosen_token)\n",
        "            if do_monosemantic and monosemantic_info is not None:\n",
        "                neighbors = monosemantic_analysis_for_token(\n",
        "                    chosen_token, model, monosemantic_info, enc, device=device, top_n=5\n",
        "                )\n",
        "                annotation_list.append((chosen_token, neighbors))\n",
        "            else:\n",
        "                annotation_list.append((chosen_token, []))\n",
        "    model.train(was_training)\n",
        "    final_text = enc.decode(context_tokens)\n",
        "    prefix_text = enc.decode(context_tokens[:-max_new_tokens])\n",
        "    annotated_strs = [prefix_text]\n",
        "    for (tid, neighs) in annotation_list:\n",
        "        token_str = enc.decode([tid])\n",
        "        if neighs:\n",
        "            neighbor_strs = [f\"{enc.decode([x[1]])}\" for x in neighs]\n",
        "            annotated = f\"{token_str}[NN={neighbor_strs}]\"\n",
        "        else:\n",
        "            annotated = token_str\n",
        "        annotated_strs.append(annotated)\n",
        "    annotated_text = \"\".join(annotated_strs)\n",
        "    return final_text, annotated_text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training Helpers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_one_model(\n",
        "    model: nn.Module,\n",
        "    loader: torch.utils.data.DataLoader,\n",
        "    epochs: int,\n",
        "    model_name: str,\n",
        "    device: torch.device,\n",
        "    lr: float = 1e-3,\n",
        "    log_steps: int = 100,\n",
        "    sample_interval: float = 30.0,\n",
        "    max_steps_per_epoch: Optional[int] = None,\n",
        "    prompt: str = \"Once upon a\",\n",
        "):\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    start_time = time.time()\n",
        "    next_sample_time = start_time\n",
        "    global_step = 0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        partial_loss = 0.0\n",
        "        partial_count = 0\n",
        "        step_in_epoch = 0\n",
        "        for batch_idx, batch_tokens in enumerate(loader, start=1):\n",
        "            step_in_epoch += 1\n",
        "            global_step += 1\n",
        "            batch_tokens = batch_tokens.to(device)\n",
        "            logits = model(batch_tokens)\n",
        "            loss = compute_next_token_loss(logits, batch_tokens)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "            partial_loss += loss.item()\n",
        "            partial_count += 1\n",
        "            if batch_idx % log_steps == 0:\n",
        "                avg_part_loss = partial_loss / partial_count\n",
        "                print(\n",
        "                    f\"[{model_name}] Epoch {epoch}/{epochs}, Step {batch_idx}/{len(loader)}\"\n",
        "                    f\" (global={global_step}) Partial Avg Loss: {avg_part_loss:.4f}\"\n",
        "                )\n",
        "                partial_loss = 0.0\n",
        "                partial_count = 0\n",
        "            current_time = time.time()\n",
        "            if current_time >= next_sample_time:\n",
        "                text_greedy, _ = generate_text(model, ENC, prompt, max_new_tokens=20, device=str(device))\n",
        "                print(f\"[{model_name}] Sample (greedy): {text_greedy}\")\n",
        "                next_sample_time = current_time + sample_interval\n",
        "            if max_steps_per_epoch is not None and step_in_epoch >= max_steps_per_epoch:\n",
        "                print(\n",
        "                    f\"[{model_name}] Reached max_steps_per_epoch={max_steps_per_epoch}, \"\n",
        "                    f\"ending epoch {epoch} early.\"\n",
        "                )\n",
        "                break\n",
        "        avg_loss = total_loss / step_in_epoch\n",
        "        print(f\"[{model_name}] *** End of Epoch {epoch} *** Avg Loss: {avg_loss:.4f}\")\n",
        "\n",
        "\n",
        "def run_experiment(config: Dict):\n",
        "    dataset, tinystories_seqs, other_seqs = load_sequences(config)\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=config[\"batch_size\"],\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=seq_collate_fn,\n",
        "    )\n",
        "    device = DEVICE if config[\"device_id\"] == \"auto\" else torch.device(config[\"device_id\"])\n",
        "    print(\n",
        "        \"Using device:\",\n",
        "        device,\n",
        "        \"block_size=\", config[\"block_size\"],\n",
        "        \"kgram_k=\", config[\"kgram_k\"],\n",
        "        \"chunk_size=\", config[\"kgram_chunk_size\"],\n",
        "        \"embed_size=\", config[\"embed_size\"],\n",
        "    )\n",
        "    models: Dict[str, nn.Module] = {}\n",
        "    if config.get(\"enable_kgram\", False):\n",
        "        variant = config.get(\"kgram_variant\", \"embedding\")\n",
        "        allow_alt = config.get(\"allow_alt_kgram\", False)\n",
        "        if variant != \"embedding\" and not allow_alt:\n",
        "            raise ValueError(\n",
        "                \"Set allow_alt_kgram=True to try the onehot or conv variants; they are gated by default.\"\n",
        "            )\n",
        "        models[\"kgram_mlp_seq\"] = KGramMLPSeqModel(\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            k=config[\"kgram_k\"],\n",
        "            embed_size=config[\"embed_size\"],\n",
        "            num_inner_layers=config[\"num_inner_layers\"],\n",
        "            chunk_size=config[\"kgram_chunk_size\"],\n",
        "            variant=variant,\n",
        "            hidden_dim=config.get(\"kgram_hidden_dim\", 512),\n",
        "            conv_hidden_dim=config.get(\"kgram_conv_hidden_dim\", 512),\n",
        "            allow_alt_variants=allow_alt,\n",
        "        ).to(device)\n",
        "    if config.get(\"enable_lstm\", True):\n",
        "        models[\"lstm_seq\"] = LSTMSeqModel(\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            embed_size=config[\"embed_size\"],\n",
        "            hidden_size=config[\"embed_size\"],\n",
        "        ).to(device)\n",
        "    if config.get(\"enable_transformer\", False):\n",
        "        models[\"transformer\"] = TransformerModel().to(device)\n",
        "    results = {}\n",
        "    for name, model in models.items():\n",
        "        print(f\"\\n=== Training model: {name} ===\")\n",
        "        train_one_model(\n",
        "            model,\n",
        "            loader,\n",
        "            epochs=config[\"epochs\"],\n",
        "            model_name=name,\n",
        "            device=device,\n",
        "            lr=config[\"learning_rate\"],\n",
        "            log_steps=config[\"log_interval_steps\"],\n",
        "            sample_interval=config[\"sample_interval_seconds\"],\n",
        "            max_steps_per_epoch=config[\"max_steps_per_epoch\"],\n",
        "            prompt=config[\"prompt\"],\n",
        "        )\n",
        "        with torch.no_grad():\n",
        "            text_greedy, ann_greedy = generate_text(\n",
        "                model, ENC, config[\"prompt\"], max_new_tokens=20, device=str(device), top_p=None\n",
        "            )\n",
        "            text_topp, ann_topp = generate_text(\n",
        "                model, ENC, config[\"prompt\"], max_new_tokens=20, device=str(device), top_p=0.95\n",
        "            )\n",
        "            text_topp1, ann_topp1 = generate_text(\n",
        "                model, ENC, config[\"prompt\"], max_new_tokens=20, device=str(device), top_p=1.0\n",
        "            )\n",
        "        results[name] = {\n",
        "            \"greedy\": (text_greedy, ann_greedy),\n",
        "            \"top_p_0.95\": (text_topp, ann_topp),\n",
        "            \"top_p_1.0\": (text_topp1, ann_topp1),\n",
        "        }\n",
        "        print(f\"[{name}] Final sample (greedy):\\n{text_greedy}\\n\")\n",
        "    print(\"\\n*** Run complete ***\")\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-gram Variant Benchmark (Optional)\n",
        "Use the cell below to compare the three architectural variants. Non-embedding variants are gated; set `allow_alt` to `True` when explicitly benchmarking them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TinyStories weight=0 => skipping TinyStories.\n",
            "Synthetic sequences: 3333\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def benchmark_kgram_variants(\n",
        "    variants=(\"embedding\", \"onehot\", \"conv\"),\n",
        "    allow_alt=False,\n",
        "    max_batches=3,\n",
        "    epochs=1,\n",
        "    batch_size=16,\n",
        "    block_size=32,\n",
        "    kgram_k=2,\n",
        "    embed_size=64,\n",
        "    num_inner_layers=1,\n",
        "    chunk_size=1,\n",
        "    learning_rate=1e-3,\n",
        "    hidden_dim=512,\n",
        "    conv_hidden_dim=512,\n",
        "):\n",
        "    \"\"\"Run a short benchmark over a handful of mini-batches for each variant.\"\"\"\n",
        "    ensure_sample_dataset()\n",
        "    dataset, _, _ = load_sequences(\n",
        "        {\n",
        "            \"tinystories_weight\": 0.0,\n",
        "            \"use_synthetic\": True,\n",
        "            \"train_subset_size\": 1,\n",
        "            \"block_size\": block_size,\n",
        "        }\n",
        "    )\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=0,\n",
        "        collate_fn=seq_collate_fn,\n",
        "    )\n",
        "    device = DEVICE\n",
        "    metrics = []\n",
        "    for variant in variants:\n",
        "        allow_flag = allow_alt if variant != \"embedding\" else False\n",
        "        model = KGramMLPSeqModel(\n",
        "            vocab_size=VOCAB_SIZE,\n",
        "            k=kgram_k,\n",
        "            embed_size=embed_size,\n",
        "            num_inner_layers=num_inner_layers,\n",
        "            chunk_size=chunk_size,\n",
        "            variant=variant,\n",
        "            hidden_dim=hidden_dim,\n",
        "            conv_hidden_dim=conv_hidden_dim,\n",
        "            allow_alt_variants=allow_flag,\n",
        "        ).to(device)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "        total_loss = 0.0\n",
        "        steps = 0\n",
        "        tokens_processed = 0\n",
        "        start = time.time()\n",
        "        for epoch in range(epochs):\n",
        "            for batch_idx, batch_tokens in enumerate(loader):\n",
        "                if batch_idx >= max_batches:\n",
        "                    break\n",
        "                batch_tokens = batch_tokens.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(batch_tokens)\n",
        "                loss = compute_next_token_loss(logits, batch_tokens)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                total_loss += loss.item()\n",
        "                steps += 1\n",
        "                tokens_processed += batch_tokens.numel()\n",
        "            else:\n",
        "                continue\n",
        "            break\n",
        "        elapsed = time.time() - start\n",
        "        metrics.append(\n",
        "            {\n",
        "                \"variant\": variant,\n",
        "                \"avg_loss\": total_loss / max(steps, 1),\n",
        "                \"tokens_per_sec\": tokens_processed / max(elapsed, 1e-6),\n",
        "                \"elapsed\": elapsed,\n",
        "                \"batches\": steps,\n",
        "                \"batch_size\": batch_size,\n",
        "            }\n",
        "        )\n",
        "        print(\n",
        "            f\"[{variant}] processed {steps} batches of size {batch_size} in {elapsed:.1f}s (tokens/s â‰ˆ {metrics[-1]['tokens_per_sec']:.1f})\"\n",
        "        )\n",
        "    return metrics\n",
        "\n",
        "def _cfg(key, default):\n",
        "    return config[key] if \"config\" in globals() and key in config else default\n",
        "\n",
        "variant_metrics = benchmark_kgram_variants(\n",
        "    allow_alt=True,\n",
        "    block_size=_cfg(\"block_size\", 32),\n",
        "    kgram_k=_cfg(\"kgram_k\", 2),\n",
        "    embed_size=_cfg(\"embed_size\", 64),\n",
        "    num_inner_layers=_cfg(\"num_inner_layers\", 1),\n",
        "    chunk_size=_cfg(\"kgram_chunk_size\", 1),\n",
        "    learning_rate=_cfg(\"learning_rate\", 1e-3),\n",
        "    hidden_dim=_cfg(\"kgram_hidden_dim\", 512),\n",
        "    conv_hidden_dim=_cfg(\"kgram_conv_hidden_dim\", 512),\n",
        "    batch_size=_cfg(\"batch_size\", 32),\n",
        ")\n",
        "print(variant_metrics)\n",
        "\n",
        "labels = [m[\"variant\"] for m in variant_metrics]\n",
        "losses = [m[\"avg_loss\"] for m in variant_metrics]\n",
        "throughputs = [m[\"tokens_per_sec\"] for m in variant_metrics]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].bar(labels, losses, color=[\"#4c72b0\", \"#55a868\", \"#c44e52\"])\n",
        "axes[0].set_title(\"Average Loss\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[1].bar(labels, throughputs, color=[\"#4c72b0\", \"#55a868\", \"#c44e52\"])\n",
        "axes[1].set_title(\"Tokens per Second\")\n",
        "axes[1].set_ylabel(\"tokens/s\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Recorded Benchmark Results\n",
        "The plot below visualizes the synthetic-corpus experiment comparing the three K-gram variants (20 mini-batches, CPU-only).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "recorded_metrics = [\n",
        "    {\"variant\": \"embedding\", \"avg_loss\": 9.4895, \"tokens_per_sec\": 42.3},\n",
        "    {\"variant\": \"conv\", \"avg_loss\": 10.2317, \"tokens_per_sec\": 32.3},\n",
        "    {\"variant\": \"onehot\", \"avg_loss\": 10.6328, \"tokens_per_sec\": 17.6},\n",
        "]\n",
        "labels = [m[\"variant\"] for m in recorded_metrics]\n",
        "losses = [m[\"avg_loss\"] for m in recorded_metrics]\n",
        "throughputs = [m[\"tokens_per_sec\"] for m in recorded_metrics]\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "axes[0].bar(labels, losses, color=[\"#4c72b0\", \"#dd8452\", \"#55a868\"], alpha=0.8)\n",
        "axes[0].set_title(\"Average Loss (Lower is Better)\")\n",
        "axes[0].set_ylabel(\"Loss\")\n",
        "axes[0].grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "axes[1].bar(labels, throughputs, color=[\"#4c72b0\", \"#dd8452\", \"#55a868\"], alpha=0.8)\n",
        "axes[1].set_title(\"Throughput (Higher is Better)\")\n",
        "axes[1].set_ylabel(\"Tokens / Second\")\n",
        "axes[1].grid(axis=\"y\", linestyle=\"--\", alpha=0.4)\n",
        "\n",
        "plt.suptitle(\"K-Gram Variant Comparison (Synthetic Dataset)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "or"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config = {\n",
        "    \"tinystories_weight\": 0.0,  # set >0 to mix in TinyStories\n",
        "    \"use_synthetic\": True,\n",
        "    \"train_subset_size\": 20000,\n",
        "    \"block_size\": 32,\n",
        "    \"batch_size\": 16,\n",
        "    \"epochs\": 3,\n",
        "    \"learning_rate\": 1e-3,\n",
        "    \"log_interval_steps\": 100,\n",
        "    \"sample_interval_seconds\": 30,\n",
        "    \"max_steps_per_epoch\": 1,\n",
        "    \"kgram_k\": 2,\n",
        "    \"kgram_chunk_size\": 1,\n",
        "    \"embed_size\": 64,\n",
        "    \"num_inner_layers\": 1,\n",
        "    \"kgram_variant\": \"embedding\",\n",
        "    \"allow_alt_kgram\": False,\n",
        "    \"kgram_hidden_dim\": 512,\n",
        "    \"kgram_conv_hidden_dim\": 512,\n",
        "    \"enable_kgram\": False,\n",
        "    \"enable_lstm\": True,\n",
        "    \"enable_transformer\": False,\n",
        "    \"prompt\": \"Once upon a\",\n",
        "    \"device_id\": \"auto\",  # or \"cpu\" / \"cuda:0\"\n",
        "}\n",
        "\n",
        "os.environ.setdefault(\"OMP_NUM_THREADS\", \"1\")\n",
        "results = run_experiment(config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n",
        "- Implement the remaining TODOs directly in this notebook (k-gram MLP, true top-$p$ sampling, RMSNorm, Transformer blocks).\n",
        "- Create additional cells for evaluation figures, loss curves, or attention visualizations.\n",
        "- Save results to Google Drive or another persistent location if you plan to revisit the session later.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
